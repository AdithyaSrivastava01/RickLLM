{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhAJBkZKiyyH"
      },
      "source": [
        "This notebook contains all the intermediate steps necessary to create the final dataset, containing the data ready to be used in Unsloth. This new dataset will be pushed to The Neural Maze organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEitO1fXjBiy"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Iyxg8t8ABRz"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltlZWg3vhNFF"
      },
      "source": [
        "# Source dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fooujMEahQ1y"
      },
      "source": [
        "The dataset we are going to build is based on `Prarabdha/Rick_and_Morty_Transcript`. The main transformation applied will be detailed in a few cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36CypqJwjo5b"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "\n",
        "dataset = load_dataset(\"Prarabdha/Rick_and_Morty_Transcript\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDWW6_ooj92g"
      },
      "outputs": [],
      "source": [
        "print(\"Number of rows: \", len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utah44qfiSpL"
      },
      "outputs": [],
      "source": [
        "dataset[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0M8AoK0h8v0"
      },
      "outputs": [],
      "source": [
        "print(dataset[10][\"dialouge\"].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WriJnit5gQRq"
      },
      "source": [
        "# Dataset Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8HcNDWbkPv-"
      },
      "source": [
        "The idea now is to generate conversations between random characters (we don't really care which one, as they will be treated as  the `user` role) and Rick Sanchez, that will assume the `assistant` role."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7xLOtVllBjU"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are an interdimensional genius scientist named Rick Sanchez.\n",
        "Be brutally honest, use sharp wit, and sprinkle in some scientific jargon.\n",
        "Don't shy away from dark humor or existential truths, but always provide a solution (even if it's unconventional).\"\"\"\n",
        "\n",
        "new_rows = []\n",
        "for i in tqdm(range(len(dataset) - 1)):\n",
        "    current_row = dataset[i]\n",
        "    next_row = dataset[i + 1]\n",
        "\n",
        "    if current_row[\"speaker\"] != 'Rick' and next_row[\"speaker\"] == 'Rick':\n",
        "        if current_row[\"episode no.\"] == next_row[\"episode no.\"]:\n",
        "            new_rows.append({\n",
        "                \"conversations_raw\": [\n",
        "                    {\"from\": \"system\", \"value\": SYSTEM_PROMPT.strip()},\n",
        "                    {\"from\": \"human\", \"value\": current_row[\"dialouge\"].strip()},\n",
        "                    {\"from\": \"gpt\", \"value\": next_row[\"dialouge\"].strip()}\n",
        "                ]\n",
        "            })\n",
        "\n",
        "sharegpt_dataset = Dataset.from_list(new_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5Y0wS758KnLo"
      },
      "outputs": [],
      "source": [
        "sharegpt_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPYiVs1DgUmr"
      },
      "source": [
        "We are going to fix the dialogues in the datasets, since there are some things not ideal for the finetuning (e.g. `:` at the beginning of some sentences, references to actions / contexts, etc.). We are going to use GPT-4o to fix all of these problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XVpCAMKg7IF"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Your task is to fix some dialogue transcripts you are going to receive.\n",
        "The idea is to remove references to actions / context, removing any\n",
        "incorrect symbols. Here are some examples:\n",
        "\n",
        "Input: stumbles in drunkenly, and turns on the lights. Morty! You gotta come on. Jus'... you gotta come with me.\n",
        "Output: Morty! You gotta come on. Jus'... you gotta come with me.\n",
        "\n",
        "Input: rubs his eyes. What, Rick? What’s going on?\n",
        "Output: What, Rick? What’s going on?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hP4k10Qla_E"
      },
      "outputs": [],
      "source": [
        "sharegpt_dataset[0][\"conversations_raw\"][1][\"value\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj08ix4wi8qh"
      },
      "outputs": [],
      "source": [
        "new_rows = []\n",
        "\n",
        "for row in tqdm(sharegpt_dataset):\n",
        "\n",
        "    rick_completion = client.chat.completions.create(\n",
        "      model=\"gpt-4o-mini\",\n",
        "      messages=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": row[\"conversations_raw\"][1][\"value\"].strip()}\n",
        "      ]\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    non_rick_completion = client.chat.completions.create(\n",
        "      model=\"gpt-4o-mini\",\n",
        "      messages=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": row[\"conversations_raw\"][2][\"value\"].strip()}\n",
        "      ]\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    new_rows.append({\n",
        "        \"conversations\": [\n",
        "            {\"from\": \"system\", \"value\": row[\"conversations_raw\"][0][\"value\"]},\n",
        "            {\"from\": \"human\", \"value\": rick_completion},\n",
        "            {\"from\": \"gpt\", \"value\": non_rick_completion}\n",
        "        ]\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK9mZzWhlZei"
      },
      "outputs": [],
      "source": [
        "sharegpt_dataset_cleaned = Dataset.from_list(new_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvvMB-ebm6jf"
      },
      "outputs": [],
      "source": [
        "sharegpt_dataset_cleaned[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLd9lgGOid5I"
      },
      "source": [
        "And that's it! We have a Dataset ready to be fed up to Unsloth following the ShareGPT style. Now, let's push the dataset to HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRNIht34io_1"
      },
      "outputs": [],
      "source": [
        "sharegpt_dataset_cleaned.push_to_hub(\"AdithyaSrivastava01/rick-and-morty-transcripts-sharegpt\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
